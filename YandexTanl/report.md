# Нагрузочное тестирование с помощью Yandex.Tank

## Ammo Generator

Код генератора можно посмотреть [тут](https://github.com/IvanovAndrey/2020-highload-dht/blob/YandexTank/src/main/java/ru/mail/polis/service/ivanovandrey/AmmoGenerator.java)

В соответствии с заданием он генерирует патроны в пяти режимах:

  Лента с PUTами с уникальными ключами
  
  Лента с PUTами с частичной перезаписью ключей (вероятность 10%)
  
  
  Лента с GETами существующих ключей с равномерным распределением (стреляем по наполненной БД)
  
  То же самое, но со смещением распределения GETов к недавно добавленным ключам (частый случай на практике)
  
  Лента со смешанной нагрузкой с 50% PUTы новых ключей и 50% GETы существующих ключей (равномерное распределение)
  
  Для определения существующих ключей был выбран примитиный принцип, опирающийся на то, что все ключи являются числами от 0 до заданного количества. 
Благодаря этому можно либо вообще не хранить листа с запросами в памяти программы, либо использовать интовую переменную.

Yandex.Tank был собран через докер, файлы load.yaml и token.txt настроены в соответствии с образцом.
Строка для запуска совпадает с приведенной в документации

```
docker run -v "$(pwd):/var/loadtest" -v "$HOME/.ssh:/root/.ssh" -it direvius/yandex-tank

```

# Полученный результат
## Выяснение нагрузки
Изначально было задано линейно растущее от времени количество запросов в секунду от 1 до 5000. Так как на не самом силбном компе запущена виртуалка, вся эта конструкция пошатнулась 
на тысяче запросов в секунду. Было принято решение стрелять по 500 константой, чтобы наверняка. Но тут пошли странности. Примерно пять сотых щапросов возвращались с кодом 71 Protocol Error/
Кластер при этом выдавал ошибку 504 Not Enough Replica. Не помогло снижение нагрузки, снижение обьема записи, даже профилирование других сервисов, которых не коснулись мои кривыые руки
выдавало те же самые ошибки 504. Получив благослаение от преподавателя я спишу этот факт на мой компьютер и выкрутасы GC. Но от этого нагрузка во всей работе была всего в 250 rps
А так же везде, где наследили запросы put около 0.05 процентов ответов будут ошибками. Таков путь...

[Результат пробной стрельбы](https://overload.yandex.net/350578#tab=test_data&tags=&plot_groups=main&machines=&metrics=&slider_start=1605523782&slider_end=1605523850&compress_ratio=1
)

## Первый этоап. Стрельба уникальными Put-запросами

![alt text](https://github.com/IvanovAndrey/2020-highload-dht/blob/YandexTank/YandexTanl/one.PNG?raw=true)

![alt text](https://github.com/IvanovAndrey/2020-highload-dht/blob/YandexTank/YandexTanl/one1.PNG?raw=true)

По времени ответа неплохо для моей старушки, по графикам тоже выглядит нормально, сервер справляется, кроме одного пика. 
Знаете, во вселенной много всего не изученного и если чего не могут понять - валят на темную материю. У меня за темную материю будет сборщик мусора. Он тоже не особо понятный и если что-то не 
так - наверное потому что он. Что-то где-то преполнилось и очищается. Либо попало в такую область памяти нод, либо я не очень знаю откуда этот пик и почему он один.

[Ссылка на результат](https://overload.yandex.net/350851#tab=test_data&tags=&plot_groups=main&machines=&metrics=&slider_start=1605539129&slider_end=1605539429)

## Второй этоап. Стрельба Put-запросами с перезаписью в одном из 10 случаев

![alt text](https://github.com/IvanovAndrey/2020-highload-dht/blob/YandexTank/YandexTanl/two.PNG?raw=true)

![alt text](https://github.com/IvanovAndrey/2020-highload-dht/blob/YandexTank/YandexTanl/two2.PNG?raw=true)

Вот тут уже более понятная мне картина. Сервер тоже справился, но перезапись очевидно более долгая операция. чем просто запись, что и видно на графике.

[Ссылка на результат](https://overload.yandex.net/350899#tab=test_data&tags=&plot_groups=main&machines=&metrics=&slider_start=1605541051&slider_end=1605541351)

## Третий этоап. Стрельба Get-запросами существующих ключей с равномерным распределением

Вот тут сначала было забавно. Сервер выдавал вот такое

![alt text](https://github.com/IvanovAndrey/2020-highload-dht/blob/YandexTank/YandexTanl/threeFail.PNG?raw=true)

Недолго подумав я понял откуда ноги растут. Количество моих патронов было рассчитано на стрельбу со скоростью сильно большей, чем итоговая. В итоге не далеко не всеми пут запросами в итоге выстрелили.
А генератор гет запросов считал что путов до него было столько же, сколько у него гетов и генерировал случайные числа из общего пула. 

Когда я пофиксил вышло вот так.

![alt text](https://github.com/IvanovAndrey/2020-highload-dht/blob/YandexTank/YandexTanl/three.PNG?raw=true)

![alt text](https://github.com/IvanovAndrey/2020-highload-dht/blob/YandexTank/YandexTanl/three3.PNG?raw=true)

Тут все выгляжит красиво, ошибки прекратили душить, все здорово, но есть пик. Не знаю откуда, темная материя, сборщик мусора и прочая аргуменация из первого пункта(

[Ссылка на результат](https://overload.yandex.net/350852#tab=test_data&tags=&plot_groups=main&machines=&metrics=&slider_start=1605539453&slider_end=1605539753
)

## Четвертый этап. Стрельба Get-запросами существующих ключей с смещением распределения 

![alt text](https://github.com/IvanovAndrey/2020-highload-dht/blob/YandexTank/YandexTanl/four.PNG?raw=true)

![alt text](https://github.com/IvanovAndrey/2020-highload-dht/blob/YandexTank/YandexTanl/four4.PNG?raw=true)

Тут у меня есть теория почему так. Вроде как для доступа к конечной записи в ноде нужно проитерироваться от первой и при распределении неравномерном, а как тут сложность операции 
меняется. Как следствие дольше, на графиках видно, но сервер справляется. Надеюсь я критически не ошибся в ствоем предположении.

[Ссылка на результат](https://overload.yandex.net/350865#tab=test_data&tags=&plot_groups=main&machines=&metrics=&slider_start=1605539823&slider_end=1605540123)

## Пятый этап. Лента со смешанной нагрузкой

![alt text](https://github.com/IvanovAndrey/2020-highload-dht/blob/YandexTank/YandexTanl/five.PNG?raw=true)

![alt text](https://github.com/IvanovAndrey/2020-highload-dht/blob/YandexTank/YandexTanl/five5.PNG?raw=true)

Тут видно во-первых что запросы действительно были двух типов, во вторых действительно их было почти поровну, а в третьих не было 404 от гетов, что значит что хорошо сработал генератор патронов.
Ну и наш старый знакомы пик вернулся, как и ошибки от путов.

[Ссылка на результат](https://overload.yandex.net/350872#tab=test_data&tags=&plot_groups=main&machines=&metrics=&slider_start=1605540163&slider_end=1605540463&compress_ratio=1)

## Анализ персентилей 

На всех этапах общий вид графиков Responsible Time Duration и Percentile похож, приведу для пятого этапа, остльные можно посмотреть по ссылкам на стрельбы
![alt text](https://github.com/IvanovAndrey/2020-highload-dht/blob/YandexTank/YandexTanl/percentile.PNG?raw=true)

По первому графику ожидалась гипербола, получилось нечто похожее, но не совсем. Отличия от идеала составляет то, что во-первых большинство запросов выполняется быстро, но 
малая часть еще быстее, а во-вторых в правом краю тоже наблюдается небольшой горбик, показывающий что пара процентов запросов выполняются сильно дольше большинства. 

Второй график тоже похож у всех эксперементов по форме. Небольшое отличие наблюдается в процентах, на которых заканчивается интенсивный рост и начинается постепенный, но почти всегда этот процент около 75.

Во вкладке Tables для всех пяти эксперементов занчение Cumulative quantiles per tag для всех запросов держится в районе 19мс для 99% (плюс минус несколько мс). Что примечательно в эксперемнте со смешанными запросами значение одинаково и для пут и для гет. 

Обобщая данную информацию можно сказать что нагрузка для сервера была вполне посильной, в большинстве случаев результаты прогнозируемые, кое где вмешиваются дополнительные факторы, такие как сборщик мусора, которые вносят небольшие изменения в общую картину.

# Профилирование
## Первый этоап. Стрельба уникальными Put-запросами

### CPU
![alt text](https://raw.githubusercontent.com/IvanovAndrey/2020-highload-dht/1a523969b80c54b32036812120630953bbe9f79b/YandexTanl/profiling/one.svg)

### Alloc
![alt text](https://raw.githubusercontent.com/IvanovAndrey/2020-highload-dht/1a523969b80c54b32036812120630953bbe9f79b/YandexTanl/profiling/one_alloc.svg)


## Второй этоап. Стрельба Put-запросами с перезаписью в одном из 10 случаев

### CPU
![alt text](https://raw.githubusercontent.com/IvanovAndrey/2020-highload-dht/1a523969b80c54b32036812120630953bbe9f79b/YandexTanl/profiling/two_cpu.svg)

### Alloc
![alt text](https://raw.githubusercontent.com/IvanovAndrey/2020-highload-dht/1a523969b80c54b32036812120630953bbe9f79b/YandexTanl/profiling/two_alloc.svg)

## Третий этоап. Стрельба Get-запросами существующих ключей с равномерным распределением

### CPU
![alt text](https://raw.githubusercontent.com/IvanovAndrey/2020-highload-dht/1a523969b80c54b32036812120630953bbe9f79b/YandexTanl/profiling/three_cpu.svg)

### Alloc
![alt text](https://raw.githubusercontent.com/IvanovAndrey/2020-highload-dht/1a523969b80c54b32036812120630953bbe9f79b/YandexTanl/profiling/three_alloc.svg)



## Четвертый этап. Стрельба Get-запросами существующих ключей с смещением распределения 

### CPU
![alt text](https://raw.githubusercontent.com/IvanovAndrey/2020-highload-dht/1a523969b80c54b32036812120630953bbe9f79b/YandexTanl/profiling/four_cpu.svg)

### Alloc
![alt text](https://raw.githubusercontent.com/IvanovAndrey/2020-highload-dht/1a523969b80c54b32036812120630953bbe9f79b/YandexTanl/profiling/four_alloc.svg)

## Пятый этап. Лента со смешанной нагрузкой

### CPU
![alt text](https://raw.githubusercontent.com/IvanovAndrey/2020-highload-dht/1a523969b80c54b32036812120630953bbe9f79b/YandexTanl/profiling/five_cpu.svg)

### Alloc
![alt text](https://raw.githubusercontent.com/IvanovAndrey/2020-highload-dht/1a523969b80c54b32036812120630953bbe9f79b/YandexTanl/profiling/five_alloc.svg)

## Анализ графов CPU

Общими особенностями каждого из графов является список вещей, на которые выделялись ресурсы процессора. Для каждого эксперемента это воркер по большей части ( кроме первого в районе 75%) и селектор.
Так как я не делал профилирования по 6му этапу могу сравнить с результатами wrk только по пятому. В сравнении с пятм этапом к воркеру и селетору добавился некто SelectorManager, который занимает порядка 10% ресурсов. Так же везде есть (но заметную часть ресурсов занимет только в первом) start_thred. Я связываю появление и того и другого с вводом механизма Compeatable Future. 

В остальном графы выглядят аналогично с профилированием под нагрузкой от wrk.

Различия между графами
В каждом эксперементе четко виден тип запроса и понять это можно по функции RocksDb. Общая тенденция - на put выделяется больше ресурсов, чем на get. особенно четко это видно в последенм эксперементе, где наблюдается разница более чем в два раза (2.73 и 1.17 процентов соответственно).

## Анализ графов Alloc

наполнение графов видим похожее, везде память делится между ворекрами (около 55%), селекторами в меньшей степени SelectorManager. Так же прослеживается внедерние технологии Compeatable Future. Относительно выделения памяти на выполнения функций put и get опять более затратным выходит put (30% против 20%).

# Итоги
Если сравнивать между собой эксперементы можно прийти к следующим выводам:
Put с перезаписью выполняется быстрее обычного. Это возможно противоречит выводам, сделанным в первой части, но если присмотреться к временным показателям на скринах, особенно сравнить показатели при 99.5 и 99 процентах это становится заметно. Обьяснить это могу только тем, что insert медленне update. Почему теперь я думаю тка - во-первых update это запись одного поля, выместо двух (value вместо key value) а во-вторых при вставке нового индекса могут происходить сопутсвующие операции (периндексация и т д).

Get из конца быстрее, чем Get с нормальным распределением.
Да, тут тоже мое мнение поменялось на 180. Но даже старые цифры говорят об этом. Почему так - только предположу: может быть имеет место кэширование, может быть добираемся к вбыранному элементу мы с конца. а не с начала. В любом случае более свежие элементы нужны сильно чаще, чем старые (что и говорилось на лекции) и такоц результат логичен

Подводя итог можно сказать что утилита в освоении не сложнее Wrk, особенно с лекцией в виде пример аперед глазами. Но в ней сильно более наглядно видно какие именно запросы приходят, и это большой плюс
(Говорю это как человек, который в первом этапе настрелял гетами по пустой базе и не заметил). И в целом утилита выглядит более наглядной и вариативной.
